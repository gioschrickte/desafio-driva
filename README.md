```markdown
# üöÄ Driva Data Pipeline Challenge

Solu√ß√£o completa de Engenharia de Dados desenvolvida para o desafio t√©cnico da Driva.
O projeto implementa um pipeline **ETL (Extract, Transform, Load)** automatizado, utilizando arquitetura de microsservi√ßos containerizados para ingest√£o, processamento, armazenamento e visualiza√ß√£o de dados de enriquecimento corporativo.

## üèóÔ∏è Arquitetura da Solu√ß√£o

O sistema foi desenhado seguindo o padr√£o de **Data Warehouse em Camadas (Bronze/Gold)**, garantindo rastreabilidade e integridade dos dados.

### Tech Stack
* **Orquestra√ß√£o & ETL:** n8n (Workflow Automation)
* **Banco de Dados:** PostgreSQL 14 (Camadas Bronze e Gold)
* **Backend/API:** Node.js + Express (Simula√ß√£o de Fonte + Analytics)
* **Frontend:** React + Vite (Dashboard)
* **Infraestrutura:** Docker & Docker Compose

### Fluxo de Dados
1.  **Fonte (Source):** API Node.js gera dados sint√©ticos de enriquecimentos (com simula√ß√£o de falhas 429 e pagina√ß√£o).
2.  **Ingest√£o (Bronze):** O n8n consome a API a cada 5 minutos, salvando o JSON bruto na tabela `bronze_enrichments`.
3.  **Processamento (Gold):** O n8n normaliza os dados, traduz para PT-BR, calcula m√©tricas de tempo (`delta_t`) e categoriza o tamanho do job, salvando na `gold_enrichments`.
4.  **Visualiza√ß√£o:** O Dashboard React consome a API de Analytics para exibir KPIs e Rankings.

---

## ‚ö° Como Executar o Projeto

### Pr√©-requisitos
* Docker e Docker Compose instalados.

### 1. Inicializa√ß√£o
Na raiz do projeto, execute o comando para construir e subir todos os cont√™ineres:

```bash
docker-compose up -d --build

```

Isso iniciar√° os seguintes servi√ßos:

* **Postgres:** Porta 5432 (Banco de dados `driva_dw`)
* **n8n:** Porta 5678 (Interface de fluxos)
* **API:** Porta 3000 (Endpoints)
* **Frontend:** Porta 5173 (Dashboard)

### 2. Acesso

* **Dashboard:** [http://localhost:5173](https://www.google.com/search?q=http://localhost:5173)
* **n8n Editor:** [http://localhost:5678](https://www.google.com/search?q=http://localhost:5678)
* **API Health:** [http://localhost:3000/analytics/overview](https://www.google.com/search?q=http://localhost:3000/analytics/overview)

---

## ‚öôÔ∏è Configura√ß√£o dos Workflows (n8n)

Para ativar o pipeline de dados, √© necess√°rio importar os fluxos de automa√ß√£o:

1. Acesse o n8n em [http://localhost:5678](https://www.google.com/search?q=http://localhost:5678).
2. Configure a credencial do **PostgreSQL** com os dados do `docker-compose.yml` (Host: `postgres`, User: `user_driva`, Pass: `password_driva`, DB: `driva_dw`).
3. Configure a credencial **Header Auth** para a API (Name: `Authorization`, Value: `Bearer driva_test_key_abc123xyz789`).
4. Importe os arquivos JSON localizados na pasta `/workflows` deste reposit√≥rio.
5. Ative o workflow **"Orquestrador"** (Switch "Active").

> **Nota:** O Orquestrador executa a cada 5 minutos. Para testar imediatamente, clique em "Execute Workflow" manualmente.

---

## üì° Documenta√ß√£o da API

A API foi constru√≠da em Node.js e possui duas responsabilidades distintas:

### 1. Simula√ß√£o de Fonte (Ingest√£o)

Simula o comportamento de um sistema externo de enriquecimento.

* **Endpoint:** `GET /people/v1/enrichments`
* **Auth:** `Bearer driva_test_key_abc123xyz789`
* **Features:**
* **Pagina√ß√£o:** Suporta `?page=X&limit=Y`.
* **Chaos Engineering:** Simula erros **429 Too Many Requests** aleatoriamente (30% de chance) para testar a resili√™ncia (Retry/Backoff) do n8n.
* **Dados Din√¢micos:** Gera datas de cria√ß√£o e atualiza√ß√£o realistas para c√°lculo de m√©tricas.



### 2. Analytics (Consumo)

Fornece dados estruturados da camada Gold para o Dashboard.

* `GET /analytics/overview`: KPIs gerais (Total, Taxa de Sucesso, Tempo M√©dio).
* `GET /analytics/enrichments`: Lista dos √∫ltimos jobs processados.
* `GET /analytics/workspaces/top` (**B√¥nus**): Ranking dos workspaces com maior volume de contatos.

---

## üß† Decis√µes de Arquitetura

Durante o desenvolvimento, as seguintes decis√µes t√©cnicas foram tomadas para atender aos requisitos:

1. **Estrat√©gia de Upsert (Idempot√™ncia):**
Tanto na camada Bronze quanto na Gold, utilizamos a opera√ß√£o de *Insert or Update* baseada no ID. Isso garante que, se o pipeline rodar duas vezes sobre os mesmos dados, n√£o haver√° duplicidade, apenas atualiza√ß√£o de estado.
2. **Resili√™ncia (Retry com Backoff):**
Como a API de fonte simula erros 429 (Rate Limit), o workflow de ingest√£o no n8n foi configurado para tentar novamente ap√≥s 2 segundos em caso de falha, garantindo robustez na coleta de dados.
3. **Separa√ß√£o de Camadas (Bronze vs Gold):**
* **Bronze:** Armazena o dado cru (`jsonb` ou texto fiel √† origem) e datas de controle (`dw_ingested_at`). O objetivo √© nunca perder o dado original.
* **Gold:** Aplica regras de neg√≥cio (tradu√ß√£o `COMPANY` -> `EMPRESA`, c√°lculo de `duracao_minutos`). O Dashboard l√™ apenas desta camada otimizada.


4. **Frontend Dockerizado:**
O Dashboard utiliza uma imagem `node:22-alpine` para compatibilidade com o Vite moderno, rodando em container para garantir que o ambiente seja agn√≥stico ao Sistema Operacional do host.

---

## üß™ Como Rodar Testes Manuais

Para validar o funcionamento via terminal (cURL):

**Testar Ingest√£o (Simulando o n8n):**

```bash
curl -v -H "Authorization: Bearer driva_test_key_abc123xyz789" \
"http://localhost:3000/people/v1/enrichments?page=1&limit=5"

```

**Testar Analytics (Simulando o Dashboard):**

```bash
curl "http://localhost:3000/analytics/workspaces/top"

```

```

---

### Passo 2: O Roteiro do V√≠deo (Baseado na Documenta√ß√£o)

Agora que voc√™ tem o README, aqui est√° como voc√™ vai usar ele para apresentar o v√≠deo. Siga essa l√≥gica para mostrar que voc√™ entendeu tudo:

**1. Abertura (O Contexto)**
* "Ol√°, sou o [Seu Nome]. Apresento minha solu√ß√£o para o desafio de Engenharia de Dados da Driva. O objetivo foi construir um pipeline robusto para monitorar processos de enriquecimento de dados."
* "Usei uma arquitetura baseada em microsservi√ßos com Docker, separando claramente as responsabilidades entre Ingest√£o, Processamento e Visualiza√ß√£o."

**2. A Infraestrutura (Mostre o Docker/VS Code)**
* *Mostre o `docker-compose.yml` rapidamente.*
* "Aqui temos o orquestrador de cont√™ineres. Temos o Postgres como Data Warehouse, o n8n para automa√ß√£o, a API Node.js e o Frontend. Tudo sobe com um √∫nico comando."

**3. O Pipeline (A Estrela do Show - Mostre o n8n)**
* *Abra o n8n na tela.*
* "O cora√ß√£o do sistema √© o n8n. No workflow de **Ingest√£o**, configurei tratamento de erro. A API simula falhas 429 (Rate Limit), e implementei uma pol√≠tica de Retry com Backoff para garantir que o dado chegue na camada Bronze."
* "No fluxo de **Processamento**, pego o dado bruto da Bronze, aplico as regras de neg√≥cio (tradu√ß√£o de campos, c√°lculo de tempo de processamento) e salvo na camada Gold."

**4. A API e Banco de Dados (Mostre o C√≥digo/DBeaver ou Terminal)**
* "No banco, segui a arquitetura medalh√£o (Bronze/Gold). A Bronze guarda o hist√≥rico fiel e a Gold entrega o dado pronto para an√°lise."
* "Desenvolvi a API com dois pap√©is: simular a fonte de dados e servir os dados anal√≠ticos."

**5. O Resultado Final (Mostre o Dashboard)**
* *Abra o http://localhost:5173.*
* "O resultado final √© consumido por este Dashboard em React. Aqui temos os KPIs de tempo m√©dio e sucesso."
* "Como b√¥nus, implementei tamb√©m o ranking de Top Workspaces, que faz uma agrega√ß√£o direta na camada Gold."

**6. Fechamento**
* "A solu√ß√£o √© totalmente containerizada, resiliente a falhas de rede e segue boas pr√°ticas de modelagem dimensional. Obrigado!"

---

### Pr√≥ximos Passos Imediatos:
1.  **Crie o arquivo README.md** com o conte√∫do acima.
2.  **Exporte os Workflows:** V√° no n8n, baixe os JSONs dos 3 workflows e salve numa pasta `workflows` no seu projeto (o README menciona isso).
3.  **Grave o v√≠deo** seguindo o roteiro. Respire fundo, fale devagar. Voc√™ construiu tudo, voc√™ sabe como funciona! üöÄ

```